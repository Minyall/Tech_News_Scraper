{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wired 3D Printer Article Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import appropriate modules\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4 import SoupStrainer\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from os.path import isfile\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = \"http://www.wired.com/?s=\"\n",
    "# the bot pretends to be a standard Mozilla browser\n",
    "hdrs = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "colnames = [\"title\", \"date\", \"desc\", \"link\", \"text\"]\n",
    "#define SoupStrainer parse filter\n",
    "parse_filter = SoupStrainer (\"div\", class_=\"pagination-container flex-row float-l-big float-l-med\")\n",
    "#print (\"Current URL to scrape is {}\").format(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to pull html content, filter out all but the specific tags, and then define 'soup'\n",
    "# Now we have a succesful response, pull the html content proper\n",
    "def get_items(thesoup):\n",
    "    lotsofitems = thesoup.find_all(\"li\")\n",
    "    thelist = []\n",
    "    for an_item in lotsofitems:\n",
    "        theitem = []\n",
    "        \n",
    "        # title\n",
    "        theitem += [an_item.find(\"h2\").get_text().encode('utf-8')]\n",
    "        # date\n",
    "        raw_the_item = an_item.find(\"time\").get_text().encode('utf-8')\n",
    "        raw_the_item = raw_the_item.strip()\n",
    "        raw_the_item = raw_the_item.replace(\" \", \"-\")\n",
    "        raw_the_item = raw_the_item.replace(\",\",\"\")\n",
    "        raw_the_item = datetime.strptime(raw_the_item, \"%B-%d-%Y\")\n",
    "        theitem += [raw_the_item]\n",
    "        # description\n",
    "        theitem += [an_item.find(\"p\").get_text().encode('utf-8')]\n",
    "        # link\n",
    "        theitem += [an_item.find('a')[\"href\"]]\n",
    "        theitem += [\"Replace-Text-here\"]\n",
    "        if not \"tbd\" in an_item:\n",
    "            thelist = thelist + [theitem]\n",
    "    return pd.DataFrame(thelist,columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# processes a beautiful_soup data structure and returns a the page count\n",
    "def get_page_count(thesoup):\n",
    "    # try to find all a tags of class \"page-numbers\"\n",
    "    page_divs = thesoup.find_all(\"a\",class_=[\"page-numbers\"])\n",
    "    page_count = 1\n",
    "    if len(page_divs)>0:\n",
    "        page_count = int(page_divs[-2].get_text())\n",
    "        return page_count\n",
    "    else:\n",
    "        print \"page count error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_text(x):\n",
    "    # art_parse_filter = SoupStrainer(\"article\", class_=\"content link-underline relative body-copy\")\n",
    "    soup = requests.get(x, headers=hdrs)\n",
    "    soup = BeautifulSoup(soup.text, \"html.parser\")\n",
    "\n",
    "    for n in soup.find_all('article'):\n",
    "        title_content = n.get_text().encode('utf-8')  #prettify text here before committing to array?\n",
    "        title_content = re.sub(\"[^a-zA-Z_0-9]\", \" \", title_content)\n",
    "        array = [title_content]\n",
    "        results = ' '.join(map(str, array))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently 0 articles in database\n"
     ]
    }
   ],
   "source": [
    "# reset the dataframe\n",
    "\n",
    "# if there already is a file...\n",
    "if isfile(\"articles.pkl\"):\n",
    "    # ...load article list from that file\n",
    "    article_db = pd.read_pickle(\"articles.pkl\")\n",
    "else:\n",
    "    # otherwise, set up an empty dataframe\n",
    "    article_db = pd.DataFrame(columns=colnames)\n",
    "\n",
    "\n",
    "# show the number of reviews in the dataframe\n",
    "print(\"Currently %d articles in database\") % (len(article_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=0&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=1&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=2&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=3&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=4&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=5&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=6&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=7&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=8&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=9&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=10&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=11&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=12&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=13&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=14&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=15&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=16&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=17&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=18&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('URL:', 'http://www.wired.com/?s=%223d+print%22&page=19&sort=date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n('URL:', 'http://www.wired.com/?s=%223d+print%22&page=20&sort=date')\n"
     ]
    }
   ],
   "source": [
    "searchterm = [str(\"%223d+print%22\")]\n",
    "# Iterate over page numbers\n",
    "for search in searchterm:\n",
    "    # initialise page_count and counter\n",
    "    page_count = 1\n",
    "    counter = 0\n",
    "    \n",
    "    while counter <= page_count:\n",
    "        \n",
    "        # 1. build the url\n",
    "        url = base_url+search+\"&page={}\".format(counter)+\"&sort=date\"\n",
    "        # 2. pull down whole page content\n",
    "        url_content = requests.get(url, headers=hdrs)\n",
    "        # 3. transform to soup using html.parser parser and parse filter\n",
    "        soup = BeautifulSoup(url_content.text, \"html.parser\", parse_only = parse_filter)\n",
    "        # 4. extract new articles from current page\n",
    "        new_items = get_items(soup)\n",
    "        # 5. add new items to the dataframe\n",
    "        article_db = article_db.append(new_items, ignore_index=True)\n",
    "        # 6. Indicate page progress\n",
    "        print(\"URL:\",url)\n",
    "        # if this is the first page then  extract the total page count\n",
    "        if counter == 0:\n",
    "            page_count = get_page_count(soup)\n",
    "        # increment counter by 1 to move to next page\n",
    "        counter += 1\n",
    "        time.sleep(5) \n",
    "        if counter == page_count:\n",
    "            # remove duplicates in case the same page has been scraped more than once\n",
    "            article_db = article_db.drop_duplicates()\n",
    "            # save the items to a csv file\n",
    "            article_db.to_csv(\"articles.csv\", sep= \",\", index_label=\"id\")\n",
    "            # save the items to a pkl file\n",
    "            article_db.to_pickle(\"articles.pkl\")\n",
    "            print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently 383 articles in the dataframe\n"
     ]
    }
   ],
   "source": [
    "total_rows = len(article_db)\n",
    "# how many reviews are there in the dataframe?\n",
    "print(\"There are currently \"+str(total_rows)+\" articles in the dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now collecting text from articles...\n"
     ]
    }
   ],
   "source": [
    "print(\"Now collecting text from articles...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired text from 1 of 383 articles: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired text from 2 of 383 articles: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired text from 3 of 383 articles: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired text from 4 of 383 articles: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired text from 5 of 383 articles: \nEnd\n"
     ]
    }
   ],
   "source": [
    "row_count = 0\n",
    "\n",
    "while row_count <= 5:\n",
    "    # 1. Identify database cell containing relevant url\n",
    "    article_url = article_db.iloc[row_count][\"link\"]\n",
    "    # 2. Run function to pull and clean text\n",
    "    article_text = pull_text(article_url)\n",
    "    if article_text is None:\n",
    "        article_text = \"XX\"\n",
    "    \n",
    "    \n",
    "    # 3. Write text to database at appropriate row in \"text column\n",
    "    article_db.loc[row_count, 'text'] = article_text\n",
    "    row_count += 1 \n",
    "    time.sleep(5) \n",
    "    print \"Acquired text from %s of %s articles: \" %(row_count, total_rows)\n",
    " \n",
    "    if row_count == 5:\n",
    "        # remove duplicates in case the same page has been scraped more than once\n",
    "        article_db = article_db.drop_duplicates()\n",
    "        # save the items to a csv file\n",
    "        article_db.to_csv(\"articles.csv\", index_label=\"id\", quotechar='\"', quoting=csv.QUOTE_ALL, escapechar=\"/\")\n",
    "        # save the items to a pkl file\n",
    "        article_db.to_pickle(\"articles.pkl\")\n",
    "        print(\"End\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text collection complete. Now showing first five entries in dataframe...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>desc</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike Ditches Safety Pins, Gives the Runner's B...</td>\n",
       "      <td>2016-07-12</td>\n",
       "      <td>Nike decided to redesign the running bib, and ...</td>\n",
       "      <td>http://www.wired.com/2016/07/nike-ditches-safe...</td>\n",
       "      <td>Whether you   re an  Olympic marathon runner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meet the Smartest, Cutest AI-Powered Robot You...</td>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>The robot revolution is coming to your living ...</td>\n",
       "      <td>http://www.wired.com/2016/06/anki-cozmo-ai-rob...</td>\n",
       "      <td>Boris Sofman taps his phone  and the robot on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Etsy Must Grow to Survive. But Can It Stay Tru...</td>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>Etsy needs to evolve without alienating its lo...</td>\n",
       "      <td>http://www.wired.com/2016/06/etsys-fight-stay-...</td>\n",
       "      <td>Inside a tightly packed studio on the fourth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevrolet Volt: Driving Progression</td>\n",
       "      <td>2016-05-23</td>\n",
       "      <td>The new Chevrolet Volt is driving progression ...</td>\n",
       "      <td>http://www.wired.com/brandlab/2016/05/chevrole...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private Jets Get the Door-Sized Windows They A...</td>\n",
       "      <td>2016-05-17</td>\n",
       "      <td>Embraer business jet customers can now orders ...</td>\n",
       "      <td>https://www.wired.com/2016/05/private-jets-get...</td>\n",
       "      <td>Flying is the great perk of the modern world ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>desc</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike Ditches Safety Pins, Gives the Runner's B...</td>\n",
       "      <td>2016-07-12</td>\n",
       "      <td>Nike decided to redesign the running bib, and ...</td>\n",
       "      <td>http://www.wired.com/2016/07/nike-ditches-safe...</td>\n",
       "      <td>Whether you   re an  Olympic marathon runner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meet the Smartest, Cutest AI-Powered Robot You...</td>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>The robot revolution is coming to your living ...</td>\n",
       "      <td>http://www.wired.com/2016/06/anki-cozmo-ai-rob...</td>\n",
       "      <td>Boris Sofman taps his phone  and the robot on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Etsy Must Grow to Survive. But Can It Stay Tru...</td>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>Etsy needs to evolve without alienating its lo...</td>\n",
       "      <td>http://www.wired.com/2016/06/etsys-fight-stay-...</td>\n",
       "      <td>Inside a tightly packed studio on the fourth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevrolet Volt: Driving Progression</td>\n",
       "      <td>2016-05-23</td>\n",
       "      <td>The new Chevrolet Volt is driving progression ...</td>\n",
       "      <td>http://www.wired.com/brandlab/2016/05/chevrole...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private Jets Get the Door-Sized Windows They A...</td>\n",
       "      <td>2016-05-17</td>\n",
       "      <td>Embraer business jet customers can now orders ...</td>\n",
       "      <td>https://www.wired.com/2016/05/private-jets-get...</td>\n",
       "      <td>Flying is the great perk of the modern world ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Text collection complete. Now showing first five entries in dataframe...\")\n",
    "\n",
    "article_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}